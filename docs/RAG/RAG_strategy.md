# RAG Strategy

## 1. Benchmark Performance and Evaluation Metrics

### Evaluation Metrics

The primary metric is Execution Accuracy, which measures whether the final answer produced by the model matches the ground truth answer (numerical or textual). In other words, execution accuracy is the final answer accuracy – the key metric for judging a system’s performance on the dataset.

They also define Program Accuracy which checks if the predicted reasoning program (sequence of operations) is semantically equivalent to the ground truth program, but the focus in many evaluations is on the final answer correctness (since there can be multiple ways to arrive at the same answer). So I will not focus on Program Accuracy in this project.

### Human and Model Performance

Answering ConvFinQA queries is challenging, even for crowd workers. Human experts achieved about 89.4% execution accuracy on a sample of questions, establishing an upper bound for performance. In contrast, non-expert crowd workers scored only ~46.9%, indicating the specialized financial reasoning required. State-of-the-art models still fall far behind expert human performance, with the best published model (FinQANet using a RoBERTa-large encoder) reaching about 68.9% execution accuracy on ConvFinQA (). This means the best model correctly answers roughly 2 out of 3 questions, leaving a significant gap to human-level performance. Even when models are given oracle (ground-truth) supporting facts, accuracy only rises to ~77% (), which suggests that comprehending the reasoning chain is difficult. Simpler generative baselines like GPT-2 or T5 fine-tuned on this task perform in the  Fifty- to Sixty-percent range, underlining that powerful strategies are needed to handle the multi-step reasoning. Overall, the final answer accuracy on ConvFinQA is a stringent benchmark – under 70% for the best models vs ~89% for experts – showing there is substantial room for improvement. Any system design should aim to improve this metric by better retrieving relevant data and correctly computing the answer.

## 2. Data Preprocessing for Text and Tables

To build a robust QA system (especially one that can do both text retrieval and text-to-SQL), the ConvFinQA data must be preprocessed into clean, structured formats:

- Parsing and Cleaning Tables: Each financial table is extracted and converted into a structured CSV format (one CSV file per table or per conversation). The table in the raw data may be provided as HTML or a list of rows; we convert this into a clean tabular format where each row represents an entry (e.g. a financial line item) and columns represent different time periods or categories. During this conversion, we remove formatting artifacts – for example, stripping out symbols like `$` or `%` and converting numeric strings into a consistent number format. This ensures that numbers can be easily compared or computed later. For instance, a cell like "$ 206588" would be stored as 206588 (an integer) in the CSV. We also capture the header or context of the table (such as the year or unit of measurement) so that queries can be mapped to the correct columns. Clean CSV tables will facilitate both direct lookup (for simple questions) and SQL queries for complex questions.
- Preparing Text Passages: The textual portions (pre_text and post_text) are concatenated or segmented into paragraphs for easier retrieval. We may split long text into smaller chunks (e.g. by paragraph or sentence) to index in the retrieval system, since shorter chunks improve search granularity. Any irrelevant text (if present) or excessively verbose parts can be pruned to focus on content likely relevant to the questions (though in ConvFinQA, the provided text is already the relevant part of the report). We also normalize the text by fixing OCR errors, standardizing terminology (e.g., "2019s" to "2019's" if needed), and ensuring consistent casing and tokenization. The goal is to have a corpus of clean textual segments that the retrieval module can index.
- Structuring for QA and SQL: We create a unified data structure that links each conversation ID to its text segments and the corresponding table data. For example, we might create a database where each report’s text paragraphs are documents and each table is stored as a separate table in a SQL database or as a CSV referenced by an ID. This dual storage is important: for text-based QA, we will use the text documents, and for structured queries, we will use the tables. We also store the ground truth answers and reasoning (from the annotations) separately for evaluation or possible supervised training (e.g. we could train a classifier to predict if a question is a “number selection” or “program” type using the provided cur_type labels.
- Consistency and Accessibility: All data is saved in readily accessible formats (e.g., JSON for the conversation metadata, CSV for tables, plain text for passages). This preprocessing yields a knowledge base where any piece of needed information – be it a numeric entry in a table or a explanatory sentence in text – can be accessed via either a search query or a database query. By cleaning and structuring the data, we reduce noise and make the subsequent retrieval and reasoning steps more reliable.

## 3. Retrieval-Augmented Generation (RAG) System Design

With the data prepared, we design a Retrieval-Augmented Generation system to answer questions using both the financial tables and text. Retrieval-Augmented Generation (RAG) is an approach where an LLM is supplemented with a retrieval step, pulling in relevant external information before generating an answer (What is RAG: Understanding Retrieval-Augmented Generation - Qdrant). This helps ensure the answers are grounded in the content (reducing hallucination) and allows the system to handle knowledge beyond the model’s parametric memory. The RAG system for ConvFinQA will work in a pipeline of retrieve-then-generate:

1. Embedding Indexing (Knowledge Base Construction): We index all the financial report content (text and tables) into a vector database (such as ChromaDB or Qdrant). To do this, we represent each piece of content as an embedding (a high-dimensional vector) using a semantic encoder. For textual segments, we can use a pre-trained sentence transformer or financial-domain BERT to obtain embeddings that capture semantic meaning. For table data, we have a couple of options:

- Flattened text representation: We can turn each table row (or each significant cell) into a textual sentence (e.g., "Net income for 2009 was 103102, for 2008 was 104222, for 2007 was 104681.") and embed it like a normal text segment. This way the vector index can directly handle table content by textual semantics (matching queries that mention "net income 2009", for example).
- Metadata-enhanced vectors: We could also store table embeddings with metadata (like column/year tags) or use specialized table encoders, but to keep it simple we treat them as text for retrieval.

2. Each indexed chunk (whether a paragraph from pre_text or a row from the table) is associated with the conversation or document ID so we know its source. The vector database allows efficient semantic search over thousands of chunks to retrieve those that are relevant to a query.
3. Semantic Retrieval: When a question comes in (potentially as part of a conversation), we first construct the query for retrieval. If it’s a conversational setting, we may prepend relevant dialogue history to the current question so that pronouns or references are resolved (e.g., "What about 2015?" on its own is ambiguous, but with history we know it might refer to "balance of goodwill in 2015"). This query is then encoded into an embedding using the same model as above. We query the vector store (Chroma/Qdrant) for the top-k most relevant chunks. The retrieval will return, for example, a few text snippets (sentences) that likely contain the answer or clues, and possibly relevant table rows if the query has numeric terms. For instance, for a question about "balance of goodwill by the end of 2014", the retrieval might return the table row for Goodwill and a sentence from the report mentioning goodwill figures. This step ensures the model has a focused context to work with, rather than the entire report. (In our experiments, retrieving around 3–5 supporting segments is a good starting point.)
4. GPT-based Answer Generation: We then construct a prompt for the generative model. This prompt can be something like: "Context: [retrieved text and data] Question: [the user’s question] Answer:". We feed this into a GPT-based model (e.g., GPT-3.5 or GPT-4 via the OpenAI API, or a similar large language model). The model, now augmented with the relevant facts in the prompt, will generate an answer. Because we provided the model with the necessary figures or statements from the annual report, it can perform the needed reasoning. For straightforward lookups, the model can directly read off the answer from the context. For arithmetic questions, the model can either do the calculation itself in the generated answer (large models are capable of simple arithmetic) or we could prompt it to explicitly show the calculation. For example, if the question asks for a percentage change, the retrieved context might include the two yearly values; the GPT model can divide the difference by the earlier value (perhaps using a chain-of-thought prompt, then output the result). The generation step should be instructed to use only the retrieved information to avoid any fabricated content. If using an advanced prompting strategy, we can ask the model to also produce a brief explanation citing the retrieved facts (though the ConvFinQA task itself typically expects just the final numeric answer).
5. Answer Output: Finally, the system returns the model’s answer. In a complete RAG application, we might also return the source references (e.g., which document or table the answer was drawn from) to increase trust. For ConvFinQA, since evaluation just checks the final answer, we focus on accuracy – but in an interactive system, showing the supporting report snippet or table excerpt can be valuable. The RAG approach leverages the strength of information retrieval to find the exact figures and statements needed, and the strength of GPT to synthesize that information and carry out reasoning. This design directly addresses the challenges seen in ConvFinQA: it ensures the model looks at the correct data (table or text) for each question, thereby improving the chance of correctness and reducing the burden on the model to remember financial facts. By integrating external retrieval, we significantly improve accuracy and relevance of answers over what the stand-alone model could do (What is RAG: Understanding Retrieval-Augmented Generation - Qdrant), especially given the large context of financial reports that is difficult to fit into the model’s prompt all at once.
6. Hybrid Query Router: Choosing Text vs. SQL Path

One key innovation for a system handling ConvFinQA is a hybrid query router – a component that decides whether a question should be answered via text-based retrieval (as above) or via a structured query (text-to-SQL) on the tabular data. This is important because some questions are best answered by reading natural language (e.g., explanations, causes, descriptions in the report text), whereas others are essentially database queries (e.g., asking for a specific number or a calculated value from the financial table). Our approach to implement the query router is as follows:

- Classification of Query Type: We analyze the incoming question (and its conversational context) to predict if it’s a “table question” or a “text question.” Simple rule-based heuristics can go a long way: if the question asks for a specific financial figure, contains numerical keywords (years, percentages, dollar amounts), or mentions words like increase, decrease, difference, total, percentage, ratio, etc., it likely requires using the structured data. For example, “What was the net income in 2019?” or “By how much did revenue grow compared to last year?” are clearly pointing to the numbers in the table (possibly with a calculation). On the other hand, if the question asks “Why” or “How” something happened, or asks for definitions or descriptions (“What does X refer to in the report?”), it will need textual evidence from the report. We can refine this by training a lightweight classifier on ConvFinQA’s training set, which is labeled per turn with a type (the dataset provides a cur_type indicating number selection vs. program (GitHub - czyssrs/ConvFinQA: Data and code for EMNLP 2022 paper "ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering")). A “number selection” question often corresponds to pulling a value from a table or text, whereas a “program” question indicates a computation (likely multiple table lookups). Such a classifier (or even few-shot prompting GPT to identify the type) can automate the routing decision.
- Routing Logic: If the router predicts a text-based query, the system uses the semantic search pipeline (Section 4) – retrieving relevant text snippets and maybe some table context – and then uses GPT to answer with a focus on explanation or factual detail from text. If the router predicts a structured-data query, the system will invoke a text-to-SQL module instead. In the text-to-SQL route, we translate the natural language question into a SQL (or Pandas) query on the preprocessed CSV table. For ConvFinQA, this is usually straightforward since the data is a single table of financial values. For example, a question like “What’s the balance of goodwill by the end of 2014?” can be mapped to a SQL query selecting the cell at row "Goodwill" and column "2014" from the appropriate table. A question like “What was the percentage change in goodwill from 2014 to 2015?” would be translated into a sequence of operations: fetch the goodwill values for 2014 and 2015, then compute the percentage difference (this could be done by SQL if the table is in a SQL database, or by fetching both values and computing in code). We can leverage the fact that our tables are in a structured format – a SQL engine or even an in-memory lookup will yield the precise numbers, eliminating potential calculation errors by the language model. Once the structured query yields an answer (e.g., a number or a list of numbers), the system can either return it directly or format it via GPT (e.g., to add units or context if needed).
- Integrating Both Paths: In some cases, a question might need both text and table, but our initial router will choose one. We design it conservatively: 
  - If a question needs a calculation, we prefer the text-to-SQL path for accuracy, because GPT can still be used afterward to phrase the answer, but the core computation is done reliably. 
  - If the question is ambiguous, the router could default to the RAG text pipeline, and the GPT model (with all retrieved info) might still handle it. 
  - For instance, if a question asks “What is that value in the subsequent year?” referring to something from a previous turn, a well-tuned router would recognize the reference to a numeric value and fetch from the table. We can also implement a fallback: if the text-based route fails to find an answer (or GPT yields low confidence), the system could try the structured route as well. Over time, learning from misclassifications (using validation data), we can refine the router’s rules or model.
In summary, the hybrid query router ensures that each question is handled by the most suitable method: natural language retrieval for explanatory or unstructured queries, and structured querying for precise numeric questions. This division of labor plays to the strengths of each approach – using SQL/database operations for exact numerical accuracy, and using the semantic understanding of GPT for open-ended or context-heavy questions. By designing the system this way, we expect to improve overall answer accuracy: straightforward data retrieval questions get answered with high precision (no hallucinated or miscalculated numbers), while complex reasoning or descriptive questions benefit from GPT’s language comprehension on the retrieved text.
Implementation Note: We can implement the router as a simple function that checks question patterns or a small classification model. During deployment, this router sits between the user query and the two pipelines (text RAG vs. structured query), directing the query appropriately. This modular design makes the RAG system flexible and robust, as it can handle the full spectrum of questions in ConvFinQA – from looking up a single value to performing multi-hop financial reasoning – by dynamically choosing the right tool for the job. Each component (retriever, generator, SQL executor, router) can be improved or replaced independently, which is useful as we iterate on the system.

---
Insights and Recommendations: By inspecting ConvFinQA and designing this system, a few insights emerge. First, data preparation is critical – having clean tables and well-segmented text greatly aids retrieval and reduces errors. Second, a pure end-to-end LLM approach is likely to struggle with the domain’s complexity and length, so a retrieval-augmented approach is necessary to achieve high accuracy. We saw that most questions require numeric reasoning, so incorporating a calculator or database execution (the text-to-SQL path) is highly beneficial for correctness. Finally, building a hybrid system with a query router addresses the mixed nature of the questions; this kind of system will likely outperform a one-size-fits-all model. By following this structured approach – understanding the data, choosing the right metrics, cleaning and organizing the knowledge, and carefully combining retrieval and generation with a routing logic – we can create a comprehensive QA solution for ConvFinQA that moves closer to expert-level performance on this challenging benchmark.